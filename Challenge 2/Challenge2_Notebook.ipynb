{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_WasRF22ixD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "58d2d35f-2f07-4b94-9de5-0b7fd3d843c3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/AN2DL/Challenge2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "id": "-6dUqGl7-x0Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tsaug\n",
    "import tsaug\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcV6U90v-2wx",
    "outputId": "84e54577-65d7-4446-95b8-67936cbf2800",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (Unzip the dataset)"
   ],
   "metadata": {
    "id": "3iLz0Cvo_BS5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip training_dataset_homework2.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPJNFAYx_DIx",
    "outputId": "5f29cdde-733e-40d5-c109-e4e1555ee31e",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seed"
   ],
   "metadata": {
    "id": "DmB6EDyz-4hJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ],
   "metadata": {
    "id": "KNufCVVC-6QQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data"
   ],
   "metadata": {
    "id": "FvgGSnhsJouc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = np.load('x_train.npy')\n",
    "y = np.load('y_train.npy')\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smctBCYKJqar",
    "outputId": "88e39582-cda8-4aa5-82b2-eb2fe0c8c221",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting the data"
   ],
   "metadata": {
    "id": "_TTeowBoAwpZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, random_state=seed,shuffle=True, test_size=0.15, stratify=y)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-6soNGu98iK",
    "outputId": "56e5aaf8-51ba-459e-d338-aa5169e5ee16",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "id": "pO0oFEyCHTW7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scaler=RobustScaler()\n",
    "x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
    "x_val = scaler.transform(x_val.reshape(-1, x_val.shape[-1])).reshape(x_val.shape)\n"
   ],
   "metadata": {
    "id": "psumwDWUn6UV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open the trained scaler"
   ],
   "metadata": {
    "id": "k5jTu_7imF3y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "filehandler = open('scaler', 'rb')\n",
    "scaler = pickle.load(filehandler)\n",
    "x_train = scaler.transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
    "x_val = scaler.transform(x_val.reshape(-1, x_val.shape[-1])).reshape(x_val.shape)"
   ],
   "metadata": {
    "id": "pafaYK335Cov"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the scaler after you train it the first time"
   ],
   "metadata": {
    "id": "wW9n4eVvmCMD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "filehandler=open('scaler', 'wb')\n",
    "pickle.dump(scaler, filehandler)"
   ],
   "metadata": {
    "id": "GoqQX_R14EM2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Augmentation"
   ],
   "metadata": {
    "id": "hxCUU6NRJhMG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train_noisy = tsaug.AddNoise(scale=0.1).augment(X=x_train)\n",
    "x_final = np.concatenate([x_train, x_train_noisy])\n",
    "print(x_final.shape)\n",
    "augmented_labels = np.tile(y_train, (2,))\n",
    "print(augmented_labels.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfqP-3M0tNZg",
    "outputId": "32af89d7-9443-4bda-e9b3-b14c1ab226a2",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the sparse labels to categorical values\n",
    "y_val = tfk.utils.to_categorical(y_val)\n",
    "augmented_labels = tfk.utils.to_categorical(augmented_labels)\n",
    "x_train.shape, y_train.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u--BF15Q-ETq",
    "outputId": "f3c98b1d-f0ea-4c9a-8225-702d3700ae34",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape = x_final.shape[1:]\n",
    "classes = augmented_labels.shape[-1]\n",
    "batch_size = 64\n",
    "epochs = 200"
   ],
   "metadata": {
    "id": "3eYHiX7Pbgdl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM Vanilla"
   ],
   "metadata": {
    "id": "vN8xfES5DL11"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_LSTM_classifier(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    lstm = tfkl.LSTM(128, return_sequences=True)(input_layer)\n",
    "    lstm = tfkl.LSTM(128)(lstm)\n",
    "    dropout = tfkl.Dropout(.5, seed=seed)(lstm)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ],
   "metadata": {
    "id": "6XtLa6WaXbib"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bilinear LSTM"
   ],
   "metadata": {
    "id": "bByrAMG3Ezw-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_BiLSTM_classifier(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    bilstm = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(input_layer)\n",
    "    bilstm = tfkl.Bidirectional(tfkl.LSTM(128))(bilstm)\n",
    "    dropout = tfkl.Dropout(.5, seed=seed)(bilstm)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ],
   "metadata": {
    "id": "Xy0H3gzqESaf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1D Convolutional"
   ],
   "metadata": {
    "id": "p2qeRTCOodZk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_1DCNN_classifier(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    cnn = tfkl.Conv1D(128,6,padding='same',activation='relu')(input_layer)\n",
    "    cnn = tfkl.MaxPooling1D()(cnn)\n",
    "    cnn = tfkl.Conv1D(128,6,padding='same',activation='relu')(cnn)\n",
    "    gap = tfkl.GlobalAveragePooling1D()(cnn)\n",
    "    dropout = tfkl.Dropout(.5, seed=seed)(gap)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ],
   "metadata": {
    "id": "oAQSA7JhohCA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention layer"
   ],
   "metadata": {
    "id": "_BajK3oZsrHb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,input_layer):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.relu(K.dot(input_layer,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = input_layer * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ],
   "metadata": {
    "id": "yQDYqkFQsstf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1D Convolutional Alternative (Best)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "YvCfrMWuuosf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_1DCNN_classifierBest(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor \n",
    "    cnn = tfkl.Conv1D(100,6,padding='same',activation='relu')(input_layer)\n",
    "    cnn = tfkl.Conv1D(100,6,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.Conv1D(100,6,padding='same',activation='relu')(cnn)\n",
    "    pool = tfkl.MaxPooling1D(pool_size=3)(cnn)\n",
    "    cnn2 = tfkl.Conv1D(100,6,padding='same',activation='relu')(pool)\n",
    "    cnn2 = tfkl.Conv1D(100,6,padding='same',activation='relu')(cnn2)\n",
    "    dropout = tfkl.Dropout(.5, seed=seed)(cnn2)\n",
    "    attention_layer = attention()(dropout)\n",
    "\n",
    "    # Classifier\n",
    "    output_layer = tfkl.Dense(classes, activation='softmax')(attention_layer)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ],
   "metadata": {
    "id": "BUjylS4RusKs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ResNet1D"
   ],
   "metadata": {
    "id": "8FBbTS1svQtv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_ResNetmodel( input_shape, nb_classes):\n",
    "        n_feature_maps = 32\n",
    "\n",
    "        input_layer = tfk.layers.Input(input_shape)\n",
    "\n",
    "        # BLOCK 1\n",
    "\n",
    "        conv_x = tfk.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "        conv_x = tfk.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = tfk.layers.Activation('relu')(conv_x)\n",
    "\n",
    "        conv_y = tfk.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = tfk.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = tfk.layers.Activation('relu')(conv_y)\n",
    "\n",
    "        conv_z = tfk.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = tfk.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "        # expand channels for the sum\n",
    "        shortcut_y = tfk.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "        shortcut_y = tfk.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        output_block_1 = tfk.layers.add([shortcut_y, conv_z])\n",
    "        output_block_1 = tfk.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "        # BLOCK 2\n",
    "\n",
    "        conv_x = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "        conv_x = tfk.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = tfk.layers.Activation('relu')(conv_x)\n",
    "\n",
    "        conv_y = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = tfk.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = tfk.layers.Activation('relu')(conv_y)\n",
    "\n",
    "        conv_z = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = tfk.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "        # expand channels for the sum\n",
    "        shortcut_y = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "        shortcut_y = tfk.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        output_block_2 = tfk.layers.add([shortcut_y, conv_z])\n",
    "        output_block_2 = tfk.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "        # BLOCK 3\n",
    "\n",
    "        conv_x = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "        conv_x = tfk.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = tfk.layers.Activation('relu')(conv_x)\n",
    "\n",
    "        conv_y = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = tfk.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = tfk.layers.Activation('relu')(conv_y)\n",
    "\n",
    "        conv_z = tfk.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = tfk.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "        # no need to expand channels because they are equal\n",
    "        shortcut_y = tfk.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "        output_block_3 = tfk.layers.add([shortcut_y, conv_z])\n",
    "        output_block_3 = tfk.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "        # FINAL\n",
    "\n",
    "        gap_layer = tfk.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "        output_layer = tfk.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "        model = tfk.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=tfk.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ],
   "metadata": {
    "id": "Oo6n52VUvTI3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the model"
   ],
   "metadata": {
    "id": "YMoaSn1100Kv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = build_1DCNN_classifierBest(input_shape, classes)\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLiiBsD_0zva",
    "outputId": "fa3acaf3-a818-415b-e645-88210499f6b5",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {
    "id": "BnNFgUwnWVMw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = x_final,\n",
    "    y = augmented_labels,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.5, min_lr=1e-5)\n",
    "    ]\n",
    ").history"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewZmQt37WYH4",
    "outputId": "db62a6a4-f243-4b5f-a188-e379c93355ac",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.save(\"1DConvBestAugAttention\")"
   ],
   "metadata": {
    "id": "xfvUINUdyn6h",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54ff205b-429a-4f72-8a2a-041848817094",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the training"
   ],
   "metadata": {
    "id": "xQn5cIv75Nz6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Binary Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "WlQbfRSV32fP",
    "outputId": "0597b6ca-d4c8-423f-c40d-609cd6e8533f",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
