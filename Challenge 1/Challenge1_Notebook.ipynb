{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjw4oV1oFfQD"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/2022_AN2DL(Private)/ChallengeDL1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAnx6eDEDzbB"
   },
   "source": [
    "#Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl81E_Q5-IY5"
   },
   "outputs": [],
   "source": [
    "# version of tensorflow needed to be able to use the ConvNeXtLarge\n",
    "!pip install tensorflow==2.10.0\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E40y_1yeECdk"
   },
   "source": [
    "#Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pztHk1BR_Nnr"
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27muyUGjELDJ"
   },
   "source": [
    "#Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxAkgO_yDmZK"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iip2rJbzEhTI"
   },
   "source": [
    "#Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxUdCy_5Er5E"
   },
   "outputs": [],
   "source": [
    "# Load the dataset to be used for classification\n",
    "!unzip training_dataset_homework1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u07ym3zxnGnX"
   },
   "outputs": [],
   "source": [
    "#Install and run splitfolder\n",
    "#Split folders: 85% of images in train folder, 15% in val folder\n",
    "!pip install split-folders\n",
    "import splitfolders\n",
    "splitfolders.ratio('training_data_final', output='Dataset', seed=seed, ratio=(0.85,0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBdFHjVpF_Vp"
   },
   "outputs": [],
   "source": [
    "# Dataset folders\n",
    "dataset_dir = 'training_data_final'\n",
    "training_dir = 'Dataset/train'\n",
    "validation_dir = 'Dataset/val'\n",
    "\n",
    "# Images are divided into folders, one for each class.\n",
    "# If the images are organized in such a way, we can exploit the\n",
    "# ImageDataGenerator to read them from disk.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator with Data Augmentation for the training data\n",
    "aug_train_data_gen = ImageDataGenerator(# after many attempts, the most suitable values ​​identified are the following:\n",
    "                                        rotation_range=30,\n",
    "                                        height_shift_range=50,\n",
    "                                        width_shift_range=50,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True,\n",
    "                                        fill_mode='reflect'\n",
    "                                        ) # rescale value is not necessary with ConvNeXtLarge, normalization is included as\n",
    "                                        # part of the model\n",
    "\n",
    "val_gen = ImageDataGenerator(#rescale value is not necessary with ConvNeXtLarge,\n",
    "    # normalization is included as part of the model\n",
    "                             )\n",
    "\n",
    "# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n",
    "#generation of training data\n",
    "train_gen = aug_train_data_gen.flow_from_directory(directory=training_dir,\n",
    "                                                       target_size=(96,96),\n",
    "                                                       color_mode='rgb',\n",
    "                                                       classes=None,\n",
    "                                                       class_mode='categorical',\n",
    "                                                       batch_size=25,\n",
    "                                                       shuffle=True,\n",
    "                                                       seed=seed)\n",
    "\n",
    "#generation of validation data\n",
    "valid_gen = val_gen.flow_from_directory(directory=validation_dir,\n",
    "                                               target_size=(96,96),\n",
    "                                               color_mode='rgb',\n",
    "                                               classes=None,\n",
    "                                               class_mode='categorical',\n",
    "                                               batch_size=25,\n",
    "                                               shuffle=False,\n",
    "                                               seed=seed)\n",
    "\n",
    "# Calculate class weights\n",
    "from collections import Counter\n",
    "counter = Counter(train_gen.classes)\n",
    "max_val = float(max(counter.values()))\n",
    "class_weights = {class_id: max_val/num_images for class_id, num_images in counter.items()}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz8049HYNu2M"
   },
   "source": [
    "#Models metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQy8eTCDNxeP"
   },
   "outputs": [],
   "source": [
    "#the size of each image in the dataset (96x96)\n",
    "input_shape = (96, 96, 3)\n",
    "\n",
    "epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMHxi3XMOT7o"
   },
   "source": [
    "#CNN model with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ugk0uaTMkR_"
   },
   "outputs": [],
   "source": [
    "# Apply same preprocessing used to train the supernet\n",
    "supernet = tfk.applications.ConvNeXtLarge(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3), #This model takes input images of shape (224, 224, 3)\n",
    "    pooling=\"avg\", #global average pooling will be applied to the output of the last convolutional layer.\n",
    "    include_preprocessing=True,\n",
    ")\n",
    "\n",
    "supernet.trainable = False #first training only to our basic model\n",
    "\n",
    "# Download and plot the model\n",
    "supernet.summary()\n",
    "tfk.utils.plot_model(supernet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYOSDIEdOVxY"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    #resizing of each image from (96,96,3) to (224,224,3)\n",
    "    x = tfkl.Resizing(224, 224, interpolation=\"bicubic\", name='resizing')(input_layer)\n",
    "\n",
    "    x = supernet(x)\n",
    "\n",
    "    #use of a classifier layer composed of 4096 units\n",
    "    classifier_layer = tfkl.Dense(units=4096, name='Classifier', kernel_initializer=tfk\n",
    "                                  .initializers.HeUniform(seed), activation='relu')(x)\n",
    "\n",
    "    #output layer classifies the 8 species\n",
    "    output_layer = tfkl.Dense(units=8, activation='softmax', kernel_initializer=tfk.initializers\n",
    "                              .GlorotUniform(seed), name='output_layer')(classifier_layer)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXyuQFWIO9uH"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY9CwxQeQByb"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit\n",
    "history = model.fit(\n",
    "    x = train_gen,\n",
    "    epochs = epochs,\n",
    "    validation_data = valid_gen,\n",
    "    class_weight = class_weights, #use of weights identified above\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=patience,\n",
    "    restore_best_weights=True)]\n",
    ").history\n",
    "\n",
    "model.save(\"Models/ConvNew3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U64q6d9pSjwO"
   },
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Binary Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG2UUWVDXcaC"
   },
   "source": [
    "#Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Re-load the model after transfer learning\n",
    "ft_model = tfk.models.load_model('Models/ConvNew3')\n",
    "netname = 'convnext_large'"
   ],
   "metadata": {
    "id": "Mz5ZZ81Y2TEv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Monlz1REXgTZ"
   },
   "outputs": [],
   "source": [
    "# Set all layers to True\n",
    "ft_model.get_layer(netname).trainable = True\n",
    "\n",
    "# Freeze first 80 layers\n",
    "for i, layer in enumerate(ft_model.get_layer(netname).layers[:80]):\n",
    "  layer.trainable=False\n",
    "\n",
    "ft_model.summary()\n",
    "\n",
    "# Compile the model with learning rate of 0.0001\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_2c2a9JX9ao"
   },
   "outputs": [],
   "source": [
    "# Train the model (fine tuning)\n",
    "ft_model.fit\n",
    "history = ft_model.fit(\n",
    "    x = train_gen,\n",
    "    epochs = epochs,\n",
    "    validation_data = valid_gen,\n",
    "    class_weight = class_weights,\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=patience,\n",
    "    restore_best_weights=True)]\n",
    ").history\n",
    "\n",
    "ft_model.save('Models/FinalModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dRVKJ64YAnO"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['loss'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Binary Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(history['accuracy'], label='Training', alpha=.8, color='#ff7f0e')\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
